# 记录要注意或要实践的东西

* [vLLM](https://qwen.readthedocs.io/zh-cn/latest/deployment/vllm.html#vllm)

    我们建议您在部署Qwen时尝试使用 [vLLM](https://github.com/vllm-project/vllm)。它易于使用，且具有最先进的服务吞吐量、高效的注意力键值内存管理（通过PagedAttention实现）、连续批处理输入请求、优化的CUDA内核等功能。要了解更多关于vLLM的信息，请参阅 [论文](https://arxiv.org/abs/2309.06180) 和 [文档](https://vllm.readthedocs.io/) 。

* [SkyPilot](https://qwen.readthedocs.io/zh-cn/latest/deployment/skypilot.html)

* [TGW](https://qwen.readthedocs.io/zh-cn/latest/web_ui/text_generation_webui.html#text-generation-web-ui)

    [Text Generation Web UI](https://github.com/oobabooga/text-generation-webui) （简称TGW，通常被称为“oobabooga”）是一款流行的文本生成Web界面工具，类似于 [AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)

    TGW 中包含了许多更多用途，您甚至可以在其中享受角色扮演的乐趣，并使用不同类型的量化模型。您可以训练诸如LoRA这样的算法，并将Stable Diffusion和Whisper等扩展功能纳入其中。


* [AWQ](https://qwen.readthedocs.io/zh-cn/latest/quantization/awq.html#awq)

    AWQ即激活值感知的权重量化(Activation-aware Weight Quantization)，是一种针对LLM的低比特权重量化的硬件友好方法。而AutoAWQ是一个易于使用的工具包，用于4比特量化模型。相较于FP16，AutoAWQ能够将模型的运行速度提升3倍，并将内存需求降低至原来的1/3。AutoAWQ实现了AWQ算法，可用于LLM的量化处理。在本文档中，我们将向您展示如何在Transformers框架下使用量化模型，以及如何对您自己的模型进行量化。